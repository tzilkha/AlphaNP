{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Diz6TXlgXmxB"
   },
   "source": [
    "# Class Exercise - MCTS with TSP\n",
    "- Tal Zilkha - tiz2102\n",
    "- Mat Hillman - mh3691"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXkT460PXmxK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "EPS = 1e-8\n",
    "import copy\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}        # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}        # stores game.getValidMoves for board s\n",
    "        \n",
    "        self.val = [float('inf')]\n",
    "        self.num_sim = [0]\n",
    "\n",
    "    def getActionProb(self, state, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(state, i)\n",
    "\n",
    "        s = self.game.stringRepresentation(state[-1])\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "        \n",
    "        if temp==0:\n",
    "            bestA = np.argmax(counts)\n",
    "            probs = [0]*len(counts)\n",
    "            probs[bestA]=1\n",
    "            return probs\n",
    "\n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x/counts_sum for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, state, num_sim):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        s = self.game.stringRepresentation(state[-1])\n",
    "        \n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(state[-1])\n",
    "        if self.Es[s]!=0:\n",
    "            # terminal node\n",
    "            return 0\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            if self.nnet is not None:\n",
    "                if args['history']:\n",
    "                    self.Ps[s], v = self.nnet.predict(state, self.game.graph)\n",
    "                else:\n",
    "                    self.Ps[s], v = self.nnet.predict(state[-1], self.game.graph)\n",
    "            else:\n",
    "                self.Ps[s] = np.ones(self.game.getActionSize()) # random policy\n",
    "                v = 0\n",
    "            valids = self.game.getValidMoves(state[-1])\n",
    "            self.Ps[s] = self.Ps[s]*valids      # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "                \n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s,a) in self.Qsa:\n",
    "                    u = self.Qsa[(s,a)] + self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + EPS)     # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, reward = self.game.getNextState(state[-1], a)\n",
    "        new_history = copy.deepcopy(state)\n",
    "        new_history.append(next_s)\n",
    "        \n",
    "        v = self.search(new_history, num_sim) + reward\n",
    "\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "            \n",
    "        if self.game.getActionSize() - self.val[-1] < v:\n",
    "            self.val += [self.game.getActionSize() - v]\n",
    "            self.num_sim += [num_sim]\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2ruv-kiXmyS"
   },
   "source": [
    "# TSPGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMskmQ-aXmyZ"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations \n",
    "\n",
    "class TSPGame():\n",
    "    \"\"\"\n",
    "    This class specifies the base Game class. To define your own game, subclass\n",
    "    this class and implement the functions below. This works when the game is\n",
    "    two-player, adversarial and turn-based.\n",
    "    Use 1 for player1 and -1 for player2.\n",
    "    See othello/OthelloGame.py for an example implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.num_node = args.num_node\n",
    "        self.graph = np.random.rand(self.num_node, 2)\n",
    "\n",
    "    def getStartState(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            start_state: a representation of the graph\n",
    "        \"\"\"\n",
    "        start_state = np.zeros([self.num_node, 2])\n",
    "        start_state[0,0] = 1\n",
    "        start_state[0,1] = 1\n",
    "        return start_state\n",
    "\n",
    "    def getActionSize(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            self.num_node: number of all possible actions\n",
    "        \"\"\"\n",
    "        return self.num_node\n",
    "\n",
    "    def getNextState(self, state, action):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            state: current state\n",
    "            action: action taken by current player\n",
    "        Returns:\n",
    "            next_state: graph after applying action\n",
    "            reward: reward from action\n",
    "        \"\"\"\n",
    "        next_state = state.copy()\n",
    "        next_state[:, 1] = 0\n",
    "        next_state[action, :] = 1\n",
    "        prev_action = np.where(state[:, 1] == 1)[0][0]\n",
    "        prev_node = self.graph[prev_action]\n",
    "        cur_node = self.graph[action]\n",
    "        reward = 1 - np.linalg.norm(cur_node - prev_node)\n",
    "        if self.num_node == np.sum(next_state[:, 0]):\n",
    "            reward += 1 - np.linalg.norm(cur_node - self.graph[0])\n",
    "            \n",
    "        return next_state, reward\n",
    "\n",
    "    def getValidMoves(self, state):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            state: current state\n",
    "        Returns:\n",
    "            1 - state[:, 0]: a binary vector of length self.getActionSize(), 1 for\n",
    "                            moves that are valid from the current board and player,\n",
    "                            0 for invalid moves\n",
    "        \"\"\"\n",
    "        return 1 - state[:, 0]\n",
    "\n",
    "    def getGameEnded(self, state):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            state: current board state\n",
    "        Returns:\n",
    "            r: 0 if game has not ended. 1 if it has\n",
    "               \n",
    "        \"\"\"\n",
    "        r = 0\n",
    "        if self.num_node == np.sum(state[:, 0]):\n",
    "            r = 1\n",
    "        return r\n",
    "\n",
    "    def stringRepresentation(self, state):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            state: current state\n",
    "        Returns:\n",
    "            s: string representation of state\n",
    "        \"\"\"\n",
    "        s = ''\n",
    "        for i in range(self.num_node):\n",
    "            s += str(int(state[i, 0]))\n",
    "        return s\n",
    "    \n",
    "    def optimal_sol(self):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            \n",
    "        Returns:\n",
    "            optimal_val: optimal solution for TSP\n",
    "            optimal_path: optimal path for TSP\n",
    "        \"\"\"\n",
    "        cur_reward = 0\n",
    "        optimal_val = float('inf')\n",
    "        optimal_path = []\n",
    "        graph = self.graph\n",
    "\n",
    "        nodes = np.arange(self.num_node)[1:]\n",
    "        perms = permutations(nodes)\n",
    "        \n",
    "        for perm in list(perms):\n",
    "            cur_reward = 0\n",
    "            \n",
    "            cur_reward += np.linalg.norm(graph[0] - graph[perm[0]])\n",
    "            for i in range(len(perm) - 1):\n",
    "                j = perm[i]\n",
    "                k = perm[i+1]\n",
    "                cur_reward += np.linalg.norm(graph[k] - graph[j])\n",
    "            cur_reward += np.linalg.norm(graph[perm[-1]] - graph[0])\n",
    "            \n",
    "            if optimal_val > cur_reward:\n",
    "                optimal_val = cur_reward\n",
    "                optimal_path = perm\n",
    "        \n",
    "        return optimal_val, optimal_path\n",
    "    \n",
    "    def create_sample(self):\n",
    "        path = self.optimal_sol()[1]+tuple([0])\n",
    "        current = self.getStartState()\n",
    "        samples_v = []\n",
    "        samples_pi = []\n",
    "        tot_v = 0\n",
    "        for i in path:\n",
    "            pi = np.zeros(self.getActionSize())\n",
    "            pi[i] = 1\n",
    "            samples_pi.append((current, pi))\n",
    "            next, v = self.getNextState(current, i)\n",
    "            samples_v.append((current, v))\n",
    "            current = next\n",
    "        \n",
    "        return samples_v, samples_pi\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77NUtw5kXm1G"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HSZvnyHXm1P"
   },
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "args = dotdict({\n",
    "    # State representation\n",
    "    'history': True,           # Whether state representation should be a history of states - used for lstm\n",
    "    'history_length': None,     # If None full history\n",
    "    \n",
    "    # MCTS args\n",
    "    'numMCTSSims': 5000,        # Number of games moves for MCTS to simulate.\n",
    "    'num_node': 5,              # Number of nodes in the graph (game)\n",
    "    \n",
    "    # Train args\n",
    "    'numIters': 1,              # Number of episods to play (5 times 10 episodes)\n",
    "    'numEps': 1,                # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,        #\n",
    "    'updateThreshold': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200000,    # Number of game examples to train the neural networks.\n",
    "    'arenaCompare': 40,         # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'numItersForTrainExamplesHistory': 25,\n",
    "    \n",
    "    # NN args\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 64,\n",
    "    'cuda': False,\n",
    "    'num_channels': 512,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbBUNyajXm11"
   },
   "source": [
    "# Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHGbBLtlXm12"
   },
   "outputs": [],
   "source": [
    "def play_game(args):\n",
    "    game = TSPGame(args)\n",
    "    mcts = MCTS(game, None, args)\n",
    "    state = [game.getStartState()]\n",
    "    mcts_reward = 0\n",
    "    mcts_actions = []\n",
    "    optimal_val, optimal_path = game.optimal_sol()\n",
    "    while not game.getGameEnded(state[-1]):\n",
    "        action = np.argmax(mcts.getActionProb(state))\n",
    "        next_state, reward = game.getNextState(state[-1], action)\n",
    "        state.append(next_state)\n",
    "        mcts_actions += [action]\n",
    "        mcts_reward += reward\n",
    "        \n",
    "#     print('Optimal Solution:', optimal_val)\n",
    "#     print('Optimal Action:', optimal_path)\n",
    "#     print('MCTS Reward:', game.getActionSize() - mcts_reward)\n",
    "#     print('MCTS Action:', mcts_actions)\n",
    "    \n",
    "    return mcts.val, mcts.num_sim, optimal_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6VCBNzSXm2i"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "1Xps_DamXm2n",
    "outputId": "89057f4e-788f-4276-f40d-2ec93cf15322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 11] [inf, 4.181742167634327, 3.9848065043247507, 3.4106279845576277, 3.138840267489805, 2.8715864803856066]\n"
     ]
    }
   ],
   "source": [
    "val, num_sim, optimal_reward = play_game(args)\n",
    "print(num_sim, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "URpz55pLXm3Z",
    "outputId": "b508f33b-de74-46b4-d7e4-a715645069ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Length')"
      ]
     },
     "execution_count": 1209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(num_sim[1:]), val[1:])\n",
    "plt.hlines(optimal_reward, 0, num_sim[-1])\n",
    "plt.xlabel(\"Number of Simulations\")\n",
    "plt.ylabel(\"Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time, os, sys\n",
    "from pickle import Pickler, Unpickler\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "class Coach():\n",
    "    \"\"\"\n",
    "    This class executes the self-play + learning. It uses the functions defined\n",
    "    in Game and NeuralNet. args are specified in main.py.\n",
    "    \"\"\"\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "#         self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "        self.trainExamplesHistory = []    # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.skipFirstSelfPlay = False    # can be overriden in loadTrainExamples()\n",
    "\n",
    "    def executeEpisode(self):\n",
    "        \"\"\"\n",
    "        This function executes one episode of self-play, starting with player 1.\n",
    "        As the game is played, each turn is added as a training example to\n",
    "        trainExamples. The game is played till the game ends. After the game\n",
    "        ends, the outcome of the game is used to assign values to each example\n",
    "        in trainExamples.\n",
    "\n",
    "        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n",
    "        uses temp=0.\n",
    "\n",
    "        Returns:\n",
    "            trainExamples: a list of examples of the form (canonicalBoard,pi,v)\n",
    "                           pi is the MCTS informed policy vector, v is +1 if\n",
    "                           the player eventually won the game, else -1.\n",
    "        \"\"\"\n",
    "        trainExamples = []\n",
    "        board = [self.game.getStartState()]\n",
    "#         self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            temp = int(episodeStep < self.args.tempThreshold)\n",
    "\n",
    "            pi = self.mcts.getActionProb(board, temp=temp)\n",
    "\n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            next_board, reward = self.game.getNextState(board[-1], action)\n",
    "            \n",
    "            trainExamples.append([board, self.game.graph, pi, reward])\n",
    "            \n",
    "            board.append(next_board)\n",
    "            \n",
    "            r = self.game.getGameEnded(board[-1])\n",
    "            \n",
    "            if r!=0:\n",
    "                return [tuple(x) for x in trainExamples]\n",
    "            \n",
    "#             board = next_board\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Performs numIters iterations with numEps episodes of self-play in each\n",
    "        iteration. After every iteration, it retrains neural network with\n",
    "        examples in trainExamples (which has a maximum length of maxlenofQueue).\n",
    "        It then pits the new neural network against the old one and accepts it\n",
    "        only if it wins >= updateThreshold fraction of games.\n",
    "        \"\"\"\n",
    "        for i in range(1, self.args.numIters+1):\n",
    "            # bookkeeping\n",
    "#             print('------ITER ' + str(i) + '------')\n",
    "            # examples of the iteration\n",
    "            if not self.skipFirstSelfPlay or i>1:\n",
    "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
    "\n",
    "                for eps in range(self.args.numEps):\n",
    "                    self.mcts = MCTS(self.game, self.nnet, self.args)   # reset search tree\n",
    "                    iterationTrainExamples += self.executeEpisode()\n",
    "\n",
    "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "                \n",
    "            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n",
    "                print(\"len(trainExamplesHistory) =\", len(self.trainExamplesHistory), \" => remove the oldest trainExamples\")\n",
    "                self.trainExamplesHistory.pop(0)\n",
    "\n",
    "            # shuffle examples before training\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            shuffle(trainExamples)\n",
    "            \n",
    "            # train\n",
    "            # self.nnet.train(trainExamples)\n",
    "        return trainExamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWO89Uw8mnTW"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiQOBbM6mnTr"
   },
   "outputs": [],
   "source": [
    "# Inputs are in the form of a merged matrix of the graph and board\n",
    "\n",
    "class ConvolutionalNN():\n",
    "    def __init__(self, args):\n",
    "        self.action_size = args['num_node']\n",
    "        self.create_net()\n",
    "        # Conv no history\n",
    "        self.args = args\n",
    "        self.args['history'] = False\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (board, pi, v)\n",
    "        \"\"\"\n",
    "        print(\"Training...\")\n",
    "        input_boards, input_graphs, target_pis, target_vs = list(zip(*examples))\n",
    "        input_boards = np.asarray([boards[-1] for boards in input_boards])\n",
    "        input_graphs = np.asarray(input_graphs)\n",
    "        target_pis = np.asarray(target_pis)\n",
    "        target_vs = np.asarray(target_vs)\n",
    "        \n",
    "        boards_graphs_list = [(input_graphs[i], input_boards[i]) for i in range(0, len(input_boards))] \n",
    "        input = [np.concatenate([*i], axis=1) for i in boards_graphs_list]\n",
    "        self.model.fit(x = [input], y = [target_pis, target_vs], \n",
    "                       batch_size = args.batch_size, epochs = args.epochs, verbose=1)\n",
    "\n",
    "    def predict(self, board, graph):\n",
    "        \"\"\"\n",
    "        board: np array with board\n",
    "        \"\"\"        \n",
    "        merged = np.concatenate([graph, board], axis=1)\n",
    "        merged = merged[np.newaxis, :, :]\n",
    "        \n",
    "        # run\n",
    "        pi, v = self.model.predict(merged)\n",
    "        return pi[0], v[0]\n",
    "    \n",
    "    def create_net(self):\n",
    "        # Neural Net\n",
    "        self.input_boards = Input(shape=(self.action_size, 4))    # s: batch_size x board_x x board_y\n",
    "\n",
    "        x_image = Reshape((self.action_size, 4, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n",
    "        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n",
    "        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='same')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n",
    "        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 2, padding='valid')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n",
    "        h_conv4_flat = Flatten()(h_conv4)       \n",
    "        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n",
    "        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n",
    "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n",
    "        self.v = Dense(1, activation='relu', name='v')(s_fc2)                    # batch_size x 1\n",
    "\n",
    "        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n",
    "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "class RecurrentNN():\n",
    "    def __init__(self, args):\n",
    "        # game params\n",
    "        self.action_size = args['num_node']\n",
    "        self.num_node = args['num_node']\n",
    "        self.board_size = (args['num_node'],2)\n",
    "        self.args = args\n",
    "        self.create_net()\n",
    "        # Conv no history\n",
    "        self.args['history'] = True\n",
    "        \n",
    "    def create_net(self):\n",
    "        self.input_boards = Input(shape=(self.num_node, self.num_node*4))\n",
    "        lstm = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(self.input_boards)\n",
    "        dense = Dense(128, activation='sigmoid')(lstm)\n",
    "        self.pi = Dense(self.action_size, activation='softmax', name='pi')(dense)\n",
    "        self.v = Dense(1, activation='relu', name='v')(dense)\n",
    "        \n",
    "        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n",
    "        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))        \n",
    "\n",
    "    def predict(self, board_list, graph):\n",
    "        input = self.prepare_input(board_list,graph)\n",
    "        pi, v = self.model.predict(input)\n",
    "\n",
    "\n",
    "        #print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return pi[0], v[0]\n",
    "    \n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (board, pi, v)\n",
    "        \"\"\"\n",
    "        print(\"Training...\")\n",
    "        input_boards, input_graphs, target_pis, target_vs = list(zip(*examples))\n",
    "        input_boards = np.asarray(input_boards)\n",
    "        input_graphs = np.asarray(input_graphs)\n",
    "        target_pis = np.asarray(target_pis)\n",
    "        target_vs = np.asarray(target_vs)\n",
    "        boards_graphs_list = [(input_boards[i], input_graphs[i]) for i in range(0, len(input_boards))] \n",
    "        prepared_inputs = np.asarray([self.prepare_input(*i)[0] for i in boards_graphs_list])\n",
    "        self.model.fit(x = [prepared_inputs], y = [target_pis, target_vs], \n",
    "                       batch_size = args.batch_size, epochs = args.epochs, verbose=1)\n",
    "\n",
    "    #\n",
    "    # State representation is a history of states with the graph appended to them and the whole thing shaped to array.\n",
    "    #\n",
    "    #      [graph, visited, camefrom]  t0\n",
    "    #      [graph, visited, camefrom]  t1\n",
    "    #      [graph, visited, camefrom]  t2\n",
    "    #      ..\n",
    "    #      ..\n",
    "    #      [graph, visited, camefrom]  tn\n",
    "    #\n",
    "    # Where tn is the current state and any timestamp that came before the first move is as such\n",
    "    #      \n",
    "    #      [graph,0,0]\n",
    "    #\n",
    "    \n",
    "    def prepare_input(self,board_list, graph):\n",
    "        if self.args['history_length'] is None:\n",
    "            self.args['history_length'] = self.num_node\n",
    "        while len(board_list)<self.args['history_length']:\n",
    "            board_list = [np.zeros(self.board_size)] + board_list\n",
    "        board_list = [x.transpose() for x in board_list]\n",
    "        input = np.array(board_list).reshape(len(board_list), self.num_node*2)\n",
    "        [graph]*self.num_node\n",
    "        graph_data = np.array(([np.array(graph).reshape(self.num_node*2)]*len(board_list))).reshape(len(board_list), self.num_node*2)\n",
    "        input = np.stack((np.array(graph_data), np.array(input)), axis=1).reshape(1,self.num_node,self.num_node*4)\n",
    "        return(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_nn(args, nn):\n",
    "    game = TSPGame(args)\n",
    "    mcts = MCTS(game, nn, args)\n",
    "    state = game.getStartState()\n",
    "    mcts_reward = 0\n",
    "    mcts_actions = []\n",
    "    optimal_val, optimal_path = game.optimal_sol()\n",
    "    \n",
    "    while not game.getGameEnded(state):\n",
    "        action = np.argmax(mcts.getActionProb(state))\n",
    "        state, reward = game.getNextState(state, action)\n",
    "        mcts_actions += [action]\n",
    "        mcts_reward += reward\n",
    "        \n",
    "#     print('Optimal Solution:', optimal_val)\n",
    "#     print('Optimal Action:', optimal_path)\n",
    "#     print('MCTS Reward:', game.getActionSize() - mcts_reward)\n",
    "#     print('MCTS Action:', mcts_actions)\n",
    "    \n",
    "    return mcts.val, mcts.num_sim, optimal_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build training dataset\n",
    "def create_training_set(args, n_games = 100):\n",
    "    data_set = []\n",
    "    for i in range(n_games):\n",
    "        t = TSPGame(args)\n",
    "        train_v, train_pi = t.create_sample()\n",
    "        for i in range(1,len(train_v)+1):\n",
    "            boards = [j[0] for j in train_v[:i]]\n",
    "            graph = t.graph\n",
    "            data_set+=[(boards, graph, train_pi[i-1][1], train_v[i-1][1])]\n",
    "    return data_set\n",
    "            \n",
    "dataset = create_training_set(args, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/200\n",
      " 5824/15000 [==========>...................] - ETA: 58s - loss: 1.9606 - pi_loss: 1.3538 - v_loss: 0.6068"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1216-514ff136e760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecurrentNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1213-20fe485409bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprepared_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboards_graphs_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         self.model.fit(x = [prepared_inputs], y = [target_pis, target_vs], \n\u001b[0;32m---> 48\u001b[0;31m                        batch_size = args.batch_size, epochs = args.epochs, verbose=1)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2859\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2861\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2862\u001b[0m     \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    659\u001b[0m   \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m       \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = RecurrentNN(args)\n",
    "rnn.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Experiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
