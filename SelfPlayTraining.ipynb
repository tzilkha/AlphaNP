{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Play Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MCTS import MCTS\n",
    "from TSP import TSPGame\n",
    "from NETS import RecurrentNN, create_training_set, ConvolutionalNN, GraphConvolutionalNN\n",
    "from EVALUATE import evaluation_run, plot_comparison, create_comparison\n",
    "from SELFPLAY import Coach\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 4\n",
    "numAssess = 30\n",
    "numEps = 300\n",
    "validation_split = 0.2\n",
    "numGens = 5\n",
    "patience = 100\n",
    "\n",
    "args = dotdict({\n",
    "    # MCTS args\n",
    "    'numMCTSSims': 100,        # Number of games moves for MCTS to simulate in self-play\n",
    "    'num_node': num_node,              # Number of nodes in the graph (game)\n",
    "    'cpuct': 1,\n",
    "    \n",
    "    # Self Play\n",
    "    'numEps': numEps,\n",
    "    'numAssess': numAssess,\n",
    "    'numGens': numGens,\n",
    "    'winThresh': 1.0           # 1 Thresh means replace on any total improvement\n",
    "\n",
    "    })\n",
    "\n",
    "rnn_args = dotdict({\n",
    "    # State representation\n",
    "    'history': True,           # Whether state representation should be a history of states - used for lstm\n",
    "    'history_length': None,     # If None full history\n",
    "    \n",
    "    # NN args\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'validation_split': validation_split,\n",
    "    'patience': patience\n",
    "    })\n",
    "\n",
    "cnn_args = dotdict({\n",
    "    # State representation\n",
    "    'history': False,           # Whether state representation should be a history of states - used for lstm\n",
    "    \n",
    "    # NN args\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'num_channels': 512,\n",
    "    'validation_split': validation_split,\n",
    "    'patience': patience\n",
    "    })\n",
    "\n",
    "gnn_args = dotdict({\n",
    "    # State representation\n",
    "    'history': False,           # Whether state representation should be a history of states - used for lstm\n",
    "    \n",
    "    # NN args\n",
    "    'lr': 0.001,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'validation_split': validation_split,\n",
    "    'patience': patience\n",
    "    })\n",
    "\n",
    "gnn_args.update(args)\n",
    "cnn_args.update(args)\n",
    "rnn_args.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 2.23 ms, total: 28.3 ms\n",
      "Wall time: 26.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create a dataset from 10000 games\n",
    "dataset = create_training_set(gnn_args, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN\n",
    "rnn = RecurrentNN(rnn_args)\n",
    "gnn = GraphConvolutionalNN(gnn_args)\n",
    "cnn = ConvolutionalNN(cnn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TZilkha/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/TZilkha/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/Columbia/Semester 9/Deep Learning/Project/AlphaNP/NETS.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mpred_choices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mloss_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_choices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2213\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gnn_hist = gnn.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gnn_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a1894da1f558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training & validation v loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnn_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgnn_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_v_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'v Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gnn_hist' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training & validation v loss values\n",
    "plt.plot(gnn_hist['v_loss'])\n",
    "plt.plot(gnn_hist['val_v_loss'])\n",
    "plt.title('v Training Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['v', 'val_v'])\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation pi loss values\n",
    "plt.plot(gnn_hist['pi_loss'])\n",
    "plt.plot(gnn_hist['val_pi_loss'])\n",
    "plt.title('pi Training Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['pi', 'val_pi'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.8095494443406615,\n",
       "  1.2707868123768633,\n",
       "  1.2234252714590814,\n",
       "  1.190780996549265,\n",
       "  1.1674777968701338,\n",
       "  1.1148690982227867,\n",
       "  1.1572553388121445,\n",
       "  1.1233019754666576,\n",
       "  1.0867860963357932,\n",
       "  1.0958481497237502,\n",
       "  1.1016012620965874,\n",
       "  1.059663784993022,\n",
       "  1.0390458519885244,\n",
       "  1.044280576216837,\n",
       "  1.0331900517476795,\n",
       "  1.062949737247817,\n",
       "  1.0523211044393952,\n",
       "  1.0311976404073246,\n",
       "  1.048448559302411,\n",
       "  1.0715057953047478,\n",
       "  1.0288801999987738,\n",
       "  1.0641067436077747,\n",
       "  1.0697892265101105,\n",
       "  1.010538234669699,\n",
       "  1.0477129700037158,\n",
       "  1.0042283495593352,\n",
       "  1.001442225468988,\n",
       "  0.9644212589950949,\n",
       "  1.0172180141800498,\n",
       "  0.9910185035955096,\n",
       "  0.9709134002849331,\n",
       "  0.9614317444981261,\n",
       "  1.019462886189762,\n",
       "  1.0179275354621553,\n",
       "  0.9113780526586247,\n",
       "  0.9736265569069188,\n",
       "  0.9409506754722033,\n",
       "  0.9662936040072753,\n",
       "  0.9625528801249262,\n",
       "  0.9192455404983985,\n",
       "  0.931266726906982,\n",
       "  0.9371931736623651,\n",
       "  0.9489020507833873,\n",
       "  0.9251317181532959,\n",
       "  0.9439842245170446,\n",
       "  0.9339568662507364,\n",
       "  0.9011872859551417,\n",
       "  0.9096894053322028,\n",
       "  0.9158548033007413,\n",
       "  0.9027491717064637,\n",
       "  0.8971246198058355,\n",
       "  0.8835097047152712,\n",
       "  0.8709983687103421,\n",
       "  0.8928458432267218,\n",
       "  0.8508223944499039,\n",
       "  0.8400850348498125,\n",
       "  0.8697800837802409,\n",
       "  0.867736943530995,\n",
       "  0.8295246888915346,\n",
       "  0.8750140927455277,\n",
       "  0.8249804060699923,\n",
       "  0.8579773930885196,\n",
       "  0.8204138008073479,\n",
       "  0.8219586406372889,\n",
       "  0.8206099131145633,\n",
       "  0.7848087286252015,\n",
       "  0.8164030105192868,\n",
       "  0.8254589354371115,\n",
       "  0.8377088322509009,\n",
       "  0.8184823823366865,\n",
       "  0.8008997254610576,\n",
       "  0.7698505337712648,\n",
       "  0.8271978862080044,\n",
       "  0.7874315487422751,\n",
       "  0.7716146945285808,\n",
       "  0.8260229432034147,\n",
       "  0.7584492160016676,\n",
       "  0.7720557276181365,\n",
       "  0.7858353538251303,\n",
       "  0.7903996336079687,\n",
       "  0.8038779233631882,\n",
       "  0.75620427559827,\n",
       "  0.7728013038461182,\n",
       "  0.7725874403635251,\n",
       "  0.7700378040544318,\n",
       "  0.7780486694367654,\n",
       "  0.7449465413367434,\n",
       "  0.7769946987451462,\n",
       "  0.7794035204752221,\n",
       "  0.7752741094571757,\n",
       "  0.7634149427693863,\n",
       "  0.7839363224657372,\n",
       "  0.7708553663994049,\n",
       "  0.755552968446765,\n",
       "  0.7957399594390561,\n",
       "  0.7683180654273609,\n",
       "  0.7647567684149349,\n",
       "  0.7519579517310503,\n",
       "  0.7859726910729298,\n",
       "  0.7870241859271779],\n",
       " 'pi_loss': [0.6275608102115339,\n",
       "  0.448524284112908,\n",
       "  0.43368410795891843,\n",
       "  0.4317340044890841,\n",
       "  0.41447086523929233,\n",
       "  0.4138549807628922,\n",
       "  0.39132442179009447,\n",
       "  0.3869377121045289,\n",
       "  0.391232641389075,\n",
       "  0.39543506033759834,\n",
       "  0.39824118190681074,\n",
       "  0.37170497744574627,\n",
       "  0.3821077920638097,\n",
       "  0.39366101976223816,\n",
       "  0.3905924633940872,\n",
       "  0.4012239107752294,\n",
       "  0.3783077340838568,\n",
       "  0.37914456330517493,\n",
       "  0.3805726331691167,\n",
       "  0.39692697612756395,\n",
       "  0.3836167239598874,\n",
       "  0.3802797760275925,\n",
       "  0.3842007894577335,\n",
       "  0.378964639392736,\n",
       "  0.37835098302365744,\n",
       "  0.3763564858749467,\n",
       "  0.3816183245038292,\n",
       "  0.3859781838207507,\n",
       "  0.3720013489216087,\n",
       "  0.3777500155751573,\n",
       "  0.39169238171344045,\n",
       "  0.37239401041058773,\n",
       "  0.3791700237581069,\n",
       "  0.37934327830360054,\n",
       "  0.38519380420845617,\n",
       "  0.3798682474201305,\n",
       "  0.36287529537377405,\n",
       "  0.3714312913509759,\n",
       "  0.37769904192674814,\n",
       "  0.3595054977385217,\n",
       "  0.37967985677367516,\n",
       "  0.37008760295138105,\n",
       "  0.3630749979903357,\n",
       "  0.37499687963771067,\n",
       "  0.37067749742394446,\n",
       "  0.37318888794069677,\n",
       "  0.3676296273159252,\n",
       "  0.3611248121515864,\n",
       "  0.3923327073954857,\n",
       "  0.3748440690097195,\n",
       "  0.3817871604260032,\n",
       "  0.3753976029059776,\n",
       "  0.35977583607758723,\n",
       "  0.37316511692604865,\n",
       "  0.36816386789548333,\n",
       "  0.36704660154682894,\n",
       "  0.3705914478668301,\n",
       "  0.3766715068789296,\n",
       "  0.36974626893250473,\n",
       "  0.3541853683883565,\n",
       "  0.36025621599861163,\n",
       "  0.37114863315778346,\n",
       "  0.36626999625998846,\n",
       "  0.3666028829308129,\n",
       "  0.37279470804148335,\n",
       "  0.3585529399842256,\n",
       "  0.3581534919821302,\n",
       "  0.37150855471072036,\n",
       "  0.3506365361197148,\n",
       "  0.3782565415785604,\n",
       "  0.37192546142760563,\n",
       "  0.35856283174173936,\n",
       "  0.373666599233878,\n",
       "  0.36571922019149533,\n",
       "  0.37505054526965625,\n",
       "  0.36001591015067935,\n",
       "  0.36601074091386454,\n",
       "  0.3618552930488737,\n",
       "  0.3606402097381772,\n",
       "  0.3568791961194139,\n",
       "  0.3661471494773226,\n",
       "  0.36162940318716574,\n",
       "  0.36844128446977426,\n",
       "  0.34912586454343525,\n",
       "  0.3749989117139539,\n",
       "  0.3611240486879036,\n",
       "  0.34107526471658983,\n",
       "  0.3523016722125506,\n",
       "  0.36434011859359383,\n",
       "  0.37094307708664565,\n",
       "  0.3522710616380725,\n",
       "  0.35444421155693373,\n",
       "  0.356185386383155,\n",
       "  0.34684430267129013,\n",
       "  0.3815332035935551,\n",
       "  0.3569636283123982,\n",
       "  0.35243072464211506,\n",
       "  0.35475349145786794,\n",
       "  0.3641945561492219,\n",
       "  0.3548958138741922],\n",
       " 'v_loss': [1.1819886341291237,\n",
       "  0.8222625282639537,\n",
       "  0.7897411635001605,\n",
       "  0.7590469920601796,\n",
       "  0.7530069316308396,\n",
       "  0.7010141174598933,\n",
       "  0.765930917022048,\n",
       "  0.736364263362128,\n",
       "  0.6955534549467172,\n",
       "  0.7004130893861517,\n",
       "  0.703360080189775,\n",
       "  0.6879588075472739,\n",
       "  0.656938059924714,\n",
       "  0.6506195564545986,\n",
       "  0.6425975883535912,\n",
       "  0.6617258264725868,\n",
       "  0.6740133703555382,\n",
       "  0.6520530771021477,\n",
       "  0.6678759261332939,\n",
       "  0.6745788191771834,\n",
       "  0.6452634760388857,\n",
       "  0.6838269675801821,\n",
       "  0.6855884370523754,\n",
       "  0.6315735952769619,\n",
       "  0.6693619869800574,\n",
       "  0.627871863684388,\n",
       "  0.6198239009651584,\n",
       "  0.5784430751743433,\n",
       "  0.6452166652584407,\n",
       "  0.613268488020351,\n",
       "  0.5792210185714921,\n",
       "  0.589037734087538,\n",
       "  0.6402928624316543,\n",
       "  0.6385842571585542,\n",
       "  0.5261842484501681,\n",
       "  0.593758309486788,\n",
       "  0.5780753800984285,\n",
       "  0.5948623126562982,\n",
       "  0.5848538381981778,\n",
       "  0.5597400427598761,\n",
       "  0.5515868701333064,\n",
       "  0.5671055707109836,\n",
       "  0.5858270527930513,\n",
       "  0.5501348385155843,\n",
       "  0.5733067270930994,\n",
       "  0.5607679783100394,\n",
       "  0.5335576586392157,\n",
       "  0.5485645931806155,\n",
       "  0.5235220959052553,\n",
       "  0.5279051026967443,\n",
       "  0.5153374593798319,\n",
       "  0.5081121018092937,\n",
       "  0.5112225326327549,\n",
       "  0.5196807263006733,\n",
       "  0.48265852655442065,\n",
       "  0.47303843330298295,\n",
       "  0.4991886359134103,\n",
       "  0.49106543665206487,\n",
       "  0.4597784199590297,\n",
       "  0.5208287243571709,\n",
       "  0.4647241900713807,\n",
       "  0.4868287599307354,\n",
       "  0.454143804547359,\n",
       "  0.45535575770647596,\n",
       "  0.44781520507307954,\n",
       "  0.4262557886409762,\n",
       "  0.45824951853715656,\n",
       "  0.45395038072639055,\n",
       "  0.48707229613118513,\n",
       "  0.44022584075812576,\n",
       "  0.4289742640334513,\n",
       "  0.411287702029525,\n",
       "  0.453531286974126,\n",
       "  0.4217123285507795,\n",
       "  0.39656414925892414,\n",
       "  0.46600703305273505,\n",
       "  0.3924384750878023,\n",
       "  0.41020043456926225,\n",
       "  0.42519514408695297,\n",
       "  0.4335204374885548,\n",
       "  0.43773077388586545,\n",
       "  0.39457487241110434,\n",
       "  0.40436001937634336,\n",
       "  0.4234615758200898,\n",
       "  0.3950388923404779,\n",
       "  0.4169246207488621,\n",
       "  0.4038712766201531,\n",
       "  0.4246930265325954,\n",
       "  0.4150634018816282,\n",
       "  0.40433103237053036,\n",
       "  0.41114388113131367,\n",
       "  0.42949211090880246,\n",
       "  0.41466998001624955,\n",
       "  0.4087086657754749,\n",
       "  0.4142067558455008,\n",
       "  0.4113544371149626,\n",
       "  0.412326043772819,\n",
       "  0.39720446027318196,\n",
       "  0.4217781349237068,\n",
       "  0.4321283720529861],\n",
       " 'val_loss': [0.36979915283382775,\n",
       "  0.4270631952593201,\n",
       "  0.3970476227664414,\n",
       "  0.428883455155055,\n",
       "  0.43773451298381827,\n",
       "  0.45249583943347466,\n",
       "  0.3912601923649117,\n",
       "  0.4190274579945575,\n",
       "  0.444555619978005,\n",
       "  0.4298841701419279,\n",
       "  0.43756962110931114,\n",
       "  0.4266004887443296,\n",
       "  0.4714660018440279,\n",
       "  0.468542478618234,\n",
       "  0.4932795725074638,\n",
       "  0.43937626380603906,\n",
       "  0.4124851979885978,\n",
       "  0.4439483192360919,\n",
       "  0.42637600677031057,\n",
       "  0.42688509389684737,\n",
       "  0.42897811499740984,\n",
       "  0.4030969009295389,\n",
       "  0.38720736239717446,\n",
       "  0.4332124116552936,\n",
       "  0.38932792540287225,\n",
       "  0.43087429407078004,\n",
       "  0.4303275917151202,\n",
       "  0.4811844600508571,\n",
       "  0.39564257767289995,\n",
       "  0.4317358599158266,\n",
       "  0.4639878007654208,\n",
       "  0.4365489003788103,\n",
       "  0.39067655261383055,\n",
       "  0.3983569186493848,\n",
       "  0.5061526372020706,\n",
       "  0.44202775933848676,\n",
       "  0.43620996534742507,\n",
       "  0.4201216276716274,\n",
       "  0.42357085509829656,\n",
       "  0.41729786121017953,\n",
       "  0.44836310651171185,\n",
       "  0.415173514056064,\n",
       "  0.3837952466971865,\n",
       "  0.42393567365280965,\n",
       "  0.39870575877238085,\n",
       "  0.3938454365667786,\n",
       "  0.40824632491599006,\n",
       "  0.3960400696191439,\n",
       "  0.41629002541980464,\n",
       "  0.41155744592210414,\n",
       "  0.4061106698387956,\n",
       "  0.4278095327552023,\n",
       "  0.39185591989667273,\n",
       "  0.4089814369054087,\n",
       "  0.41199432926305796,\n",
       "  0.40869170369776187,\n",
       "  0.38866153262044456,\n",
       "  0.388434812543311,\n",
       "  0.4088369038055335,\n",
       "  0.32529681812937283,\n",
       "  0.3913434393792611,\n",
       "  0.3630450673415723,\n",
       "  0.38458690419658026,\n",
       "  0.38937627157696947,\n",
       "  0.3960743590196678,\n",
       "  0.3940949083150639,\n",
       "  0.365679285488671,\n",
       "  0.3799368525527972,\n",
       "  0.33439522764276297,\n",
       "  0.38891425442492294,\n",
       "  0.4001096758765423,\n",
       "  0.40807495889957207,\n",
       "  0.3773430946304688,\n",
       "  0.3995263783714903,\n",
       "  0.42000801992715403,\n",
       "  0.36318822101397336,\n",
       "  0.41929621167138525,\n",
       "  0.39441105973387064,\n",
       "  0.3709920954686942,\n",
       "  0.38525424500619637,\n",
       "  0.38301800526029706,\n",
       "  0.4192602443221467,\n",
       "  0.41263630257997913,\n",
       "  0.3724001363948635,\n",
       "  0.4133316472006187,\n",
       "  0.3845823220596112,\n",
       "  0.3799697951799864,\n",
       "  0.3714009515110884,\n",
       "  0.3853964294546034,\n",
       "  0.3953004332542559,\n",
       "  0.38123566760276256,\n",
       "  0.3643159022224785,\n",
       "  0.37631809166570834,\n",
       "  0.3678629849443633,\n",
       "  0.39379808979165715,\n",
       "  0.37370994732152896,\n",
       "  0.37831158070216575,\n",
       "  0.3963942096220843,\n",
       "  0.3765453390056385,\n",
       "  0.3526107009744247],\n",
       " 'val_pi_loss': [0.22170869773903015,\n",
       "  0.24963288142118958,\n",
       "  0.24053485532990881,\n",
       "  0.25764108917844053,\n",
       "  0.2628972698592839,\n",
       "  0.2635160982900828,\n",
       "  0.2518126096239099,\n",
       "  0.2537891272500971,\n",
       "  0.26391896417432975,\n",
       "  0.2665985430848342,\n",
       "  0.26837650861396356,\n",
       "  0.2559912773114739,\n",
       "  0.26483964398296916,\n",
       "  0.2747304329481596,\n",
       "  0.2714994360869113,\n",
       "  0.2764282345989338,\n",
       "  0.2594440591309103,\n",
       "  0.2604087805687849,\n",
       "  0.2660264642493825,\n",
       "  0.2819146907937741,\n",
       "  0.2666942625889145,\n",
       "  0.2668114758797574,\n",
       "  0.2677525752711347,\n",
       "  0.2672523591031126,\n",
       "  0.2688454380538008,\n",
       "  0.2640133200102724,\n",
       "  0.27454615702045493,\n",
       "  0.2794064796016686,\n",
       "  0.263616442056935,\n",
       "  0.2669086502801595,\n",
       "  0.28431597383646795,\n",
       "  0.2662655827014089,\n",
       "  0.27345325082275546,\n",
       "  0.2793839587771236,\n",
       "  0.2798636105462903,\n",
       "  0.27717586955771917,\n",
       "  0.2727122878774287,\n",
       "  0.2743326120531604,\n",
       "  0.2780709214449282,\n",
       "  0.2678251880312252,\n",
       "  0.283668473288359,\n",
       "  0.27637509611979644,\n",
       "  0.26451206835446295,\n",
       "  0.27863237238452715,\n",
       "  0.2747942515394342,\n",
       "  0.27538304540672986,\n",
       "  0.27577510699207414,\n",
       "  0.26552958334503746,\n",
       "  0.2902008261569392,\n",
       "  0.27965783534035926,\n",
       "  0.2875515025000756,\n",
       "  0.2879381662880222,\n",
       "  0.2715344022087942,\n",
       "  0.2788226567050204,\n",
       "  0.2757191757903224,\n",
       "  0.2789575212853805,\n",
       "  0.27910152618680295,\n",
       "  0.28621474530757784,\n",
       "  0.28227315792819263,\n",
       "  0.2616821091631279,\n",
       "  0.27116715109904455,\n",
       "  0.2771339513901301,\n",
       "  0.2782277738855263,\n",
       "  0.27712725481002565,\n",
       "  0.2834031378380507,\n",
       "  0.2683330339072195,\n",
       "  0.2726064458220543,\n",
       "  0.2886170646757578,\n",
       "  0.2698538061156142,\n",
       "  0.28850535093625235,\n",
       "  0.2846213542081276,\n",
       "  0.28149623357601666,\n",
       "  0.2902907221135308,\n",
       "  0.2767741810259766,\n",
       "  0.28624628600403074,\n",
       "  0.2753388523951894,\n",
       "  0.28568188541977796,\n",
       "  0.2740059323199407,\n",
       "  0.2761861342615643,\n",
       "  0.2755893942971378,\n",
       "  0.2851851793631893,\n",
       "  0.28677383598784195,\n",
       "  0.2847819474242616,\n",
       "  0.2714967357985685,\n",
       "  0.2905410433582832,\n",
       "  0.2794980882342785,\n",
       "  0.264878355329472,\n",
       "  0.27408165527247713,\n",
       "  0.28452802851420017,\n",
       "  0.29363966458778834,\n",
       "  0.2763820430634807,\n",
       "  0.2713475258740178,\n",
       "  0.2794571852147509,\n",
       "  0.27017031115235374,\n",
       "  0.29742028853536456,\n",
       "  0.2734148293266221,\n",
       "  0.2810990372285911,\n",
       "  0.2776073847617028,\n",
       "  0.28966236415111335,\n",
       "  0.27562433445839063],\n",
       " 'val_v_loss': [0.1480904550947976,\n",
       "  0.17743031383813052,\n",
       "  0.15651276743653242,\n",
       "  0.17124236597661438,\n",
       "  0.17483724312453425,\n",
       "  0.18897974114339183,\n",
       "  0.13944758274100189,\n",
       "  0.1652383307444604,\n",
       "  0.18063665580367505,\n",
       "  0.16328562705709362,\n",
       "  0.1691931124953476,\n",
       "  0.17060921143285573,\n",
       "  0.20662635786105893,\n",
       "  0.1938120456700746,\n",
       "  0.22178013642055253,\n",
       "  0.1629480292071051,\n",
       "  0.15304113885768736,\n",
       "  0.18353953866730682,\n",
       "  0.1603495425209282,\n",
       "  0.1449704031030731,\n",
       "  0.1622838524084952,\n",
       "  0.13628542504978167,\n",
       "  0.1194547871260397,\n",
       "  0.1659600525521808,\n",
       "  0.12048248734907156,\n",
       "  0.1668609740605077,\n",
       "  0.1557814346946652,\n",
       "  0.2017779804491885,\n",
       "  0.13202613561596488,\n",
       "  0.16482720963566705,\n",
       "  0.17967182692895292,\n",
       "  0.1702833176774014,\n",
       "  0.11722330179107518,\n",
       "  0.11897295987226111,\n",
       "  0.22628902665578032,\n",
       "  0.1648518897807674,\n",
       "  0.16349767746999663,\n",
       "  0.14578901561846697,\n",
       "  0.14549993365336839,\n",
       "  0.1494726731789541,\n",
       "  0.16469463322335293,\n",
       "  0.13879841793626738,\n",
       "  0.11928317834272352,\n",
       "  0.14530330126828245,\n",
       "  0.1239115072329466,\n",
       "  0.1184623911600486,\n",
       "  0.13247121792391592,\n",
       "  0.13051048627410647,\n",
       "  0.12608919926286546,\n",
       "  0.13189961058174485,\n",
       "  0.11855916733871993,\n",
       "  0.13987136646718007,\n",
       "  0.12032151768787827,\n",
       "  0.13015878020038826,\n",
       "  0.13627515347273533,\n",
       "  0.12973418241238113,\n",
       "  0.1095600064336416,\n",
       "  0.10222006723573315,\n",
       "  0.1265637458773409,\n",
       "  0.0636147089662449,\n",
       "  0.12017628828021648,\n",
       "  0.08591111595144213,\n",
       "  0.10635913031105389,\n",
       "  0.11224901676694386,\n",
       "  0.1126712211816173,\n",
       "  0.12576187440784423,\n",
       "  0.09307283966661654,\n",
       "  0.09131978787703937,\n",
       "  0.06454142152714884,\n",
       "  0.10040890348867056,\n",
       "  0.11548832166841465,\n",
       "  0.1265787253235553,\n",
       "  0.08705237251693779,\n",
       "  0.1227521973455137,\n",
       "  0.1337617339231233,\n",
       "  0.08784936861878391,\n",
       "  0.13361432625160727,\n",
       "  0.12040512741393004,\n",
       "  0.0948059612071298,\n",
       "  0.10966485070905861,\n",
       "  0.09783282589710775,\n",
       "  0.13248640833430486,\n",
       "  0.12785435515571758,\n",
       "  0.10090340059629524,\n",
       "  0.12279060384233552,\n",
       "  0.10508423382533265,\n",
       "  0.11509143985051425,\n",
       "  0.09731929623861114,\n",
       "  0.10086840094040309,\n",
       "  0.10166076866646748,\n",
       "  0.10485362453928186,\n",
       "  0.09296837634846061,\n",
       "  0.0968609064509574,\n",
       "  0.09769267379200966,\n",
       "  0.0963778012562926,\n",
       "  0.10029511799490681,\n",
       "  0.09721254347357469,\n",
       "  0.11878682486038142,\n",
       "  0.08688297485452526,\n",
       "  0.07698636651603419]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Coach(cnn, rnn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Assessing old...\n",
      "Dataset Size 900\n",
      "Training...\n",
      "Train on 720 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 2.2007 - pi_loss: 0.8596 - v_loss: 1.3411 - val_loss: 1.6418 - val_pi_loss: 1.1272 - val_v_loss: 0.5146\n",
      "Epoch 2/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1342 - pi_loss: 0.6031 - v_loss: 0.5311 - val_loss: 1.5552 - val_pi_loss: 0.9936 - val_v_loss: 0.5616\n",
      "Epoch 3/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1755 - pi_loss: 0.6161 - v_loss: 0.5595 - val_loss: 1.5464 - val_pi_loss: 0.9847 - val_v_loss: 0.5616\n",
      "Epoch 4/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1375 - pi_loss: 0.5826 - v_loss: 0.5549 - val_loss: 1.5545 - val_pi_loss: 0.9929 - val_v_loss: 0.5616\n",
      "Epoch 5/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1388 - pi_loss: 0.5696 - v_loss: 0.5692 - val_loss: 1.6270 - val_pi_loss: 1.0653 - val_v_loss: 0.5616\n",
      "Epoch 6/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1409 - pi_loss: 0.5778 - v_loss: 0.5631 - val_loss: 1.5889 - val_pi_loss: 1.0273 - val_v_loss: 0.5616\n",
      "Epoch 7/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.1027 - pi_loss: 0.5693 - v_loss: 0.5333 - val_loss: 1.6283 - val_pi_loss: 1.0667 - val_v_loss: 0.5616\n",
      "Epoch 8/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.0893 - pi_loss: 0.5363 - v_loss: 0.5530 - val_loss: 1.6326 - val_pi_loss: 1.0710 - val_v_loss: 0.5616\n",
      "Epoch 9/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 1.0239 - pi_loss: 0.5231 - v_loss: 0.5008 - val_loss: 1.6467 - val_pi_loss: 1.0850 - val_v_loss: 0.5616\n",
      "Epoch 10/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.0299 - pi_loss: 0.5233 - v_loss: 0.5066 - val_loss: 1.7038 - val_pi_loss: 1.1422 - val_v_loss: 0.5616\n",
      "Epoch 11/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.0399 - pi_loss: 0.5109 - v_loss: 0.5290 - val_loss: 1.6615 - val_pi_loss: 1.0999 - val_v_loss: 0.5616\n",
      "Epoch 12/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 1.0611 - pi_loss: 0.5404 - v_loss: 0.5208 - val_loss: 1.7598 - val_pi_loss: 1.2005 - val_v_loss: 0.5593\n",
      "Epoch 13/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 1.0246 - pi_loss: 0.5129 - v_loss: 0.5117 - val_loss: 1.6536 - val_pi_loss: 1.1537 - val_v_loss: 0.4999\n",
      "Epoch 14/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9717 - pi_loss: 0.4937 - v_loss: 0.4779 - val_loss: 1.6444 - val_pi_loss: 1.1932 - val_v_loss: 0.4512\n",
      "Epoch 15/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9792 - pi_loss: 0.4831 - v_loss: 0.4961 - val_loss: 1.7137 - val_pi_loss: 1.1568 - val_v_loss: 0.5569\n",
      "Epoch 16/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9792 - pi_loss: 0.4854 - v_loss: 0.4938 - val_loss: 1.6304 - val_pi_loss: 1.1520 - val_v_loss: 0.4784\n",
      "Epoch 17/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9350 - pi_loss: 0.4815 - v_loss: 0.4535 - val_loss: 1.6190 - val_pi_loss: 1.1769 - val_v_loss: 0.4420\n",
      "Epoch 18/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9678 - pi_loss: 0.5035 - v_loss: 0.4642 - val_loss: 1.4941 - val_pi_loss: 1.1696 - val_v_loss: 0.3246\n",
      "Epoch 19/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9116 - pi_loss: 0.4704 - v_loss: 0.4413 - val_loss: 1.5081 - val_pi_loss: 1.2220 - val_v_loss: 0.2861\n",
      "Epoch 20/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.9106 - pi_loss: 0.4927 - v_loss: 0.4179 - val_loss: 1.4569 - val_pi_loss: 1.2166 - val_v_loss: 0.2403\n",
      "Epoch 21/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8888 - pi_loss: 0.4780 - v_loss: 0.4108 - val_loss: 1.4896 - val_pi_loss: 1.2764 - val_v_loss: 0.2132\n",
      "Epoch 22/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8843 - pi_loss: 0.4713 - v_loss: 0.4130 - val_loss: 1.4830 - val_pi_loss: 1.2794 - val_v_loss: 0.2036\n",
      "Epoch 23/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8432 - pi_loss: 0.4526 - v_loss: 0.3906 - val_loss: 1.4544 - val_pi_loss: 1.2315 - val_v_loss: 0.2229\n",
      "Epoch 24/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8586 - pi_loss: 0.4441 - v_loss: 0.4146 - val_loss: 1.4584 - val_pi_loss: 1.2285 - val_v_loss: 0.2300\n",
      "Epoch 25/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8071 - pi_loss: 0.4347 - v_loss: 0.3724 - val_loss: 1.4995 - val_pi_loss: 1.2507 - val_v_loss: 0.2488\n",
      "Epoch 26/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8206 - pi_loss: 0.4374 - v_loss: 0.3832 - val_loss: 1.4722 - val_pi_loss: 1.2475 - val_v_loss: 0.2247\n",
      "Epoch 27/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8249 - pi_loss: 0.4456 - v_loss: 0.3792 - val_loss: 1.4606 - val_pi_loss: 1.2274 - val_v_loss: 0.2331\n",
      "Epoch 28/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.8157 - pi_loss: 0.4379 - v_loss: 0.3778 - val_loss: 1.4624 - val_pi_loss: 1.2420 - val_v_loss: 0.2204\n",
      "Epoch 29/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7922 - pi_loss: 0.4178 - v_loss: 0.3744 - val_loss: 1.4369 - val_pi_loss: 1.2137 - val_v_loss: 0.2233\n",
      "Epoch 30/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7941 - pi_loss: 0.4256 - v_loss: 0.3685 - val_loss: 1.4706 - val_pi_loss: 1.2452 - val_v_loss: 0.2254\n",
      "Epoch 31/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7805 - pi_loss: 0.4140 - v_loss: 0.3665 - val_loss: 1.3518 - val_pi_loss: 1.1735 - val_v_loss: 0.1784\n",
      "Epoch 32/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7602 - pi_loss: 0.4059 - v_loss: 0.3544 - val_loss: 1.3725 - val_pi_loss: 1.1713 - val_v_loss: 0.2012\n",
      "Epoch 33/100\n",
      "720/720 [==============================] - 3s 4ms/step - loss: 0.7506 - pi_loss: 0.4022 - v_loss: 0.3484 - val_loss: 1.3089 - val_pi_loss: 1.1544 - val_v_loss: 0.1544\n",
      "Epoch 34/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.7342 - pi_loss: 0.3974 - v_loss: 0.3368 - val_loss: 1.3161 - val_pi_loss: 1.1951 - val_v_loss: 0.1210\n",
      "Epoch 35/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.7606 - pi_loss: 0.4084 - v_loss: 0.3522 - val_loss: 1.2924 - val_pi_loss: 1.0836 - val_v_loss: 0.2087\n",
      "Epoch 36/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.7392 - pi_loss: 0.3988 - v_loss: 0.3404 - val_loss: 1.3060 - val_pi_loss: 1.0726 - val_v_loss: 0.2334\n",
      "Epoch 37/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.7190 - pi_loss: 0.3918 - v_loss: 0.3272 - val_loss: 1.2054 - val_pi_loss: 0.9811 - val_v_loss: 0.2244\n",
      "Epoch 38/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.7309 - pi_loss: 0.3972 - v_loss: 0.3336 - val_loss: 1.1912 - val_pi_loss: 0.9479 - val_v_loss: 0.2433\n",
      "Epoch 39/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.7262 - pi_loss: 0.3941 - v_loss: 0.3320 - val_loss: 1.1460 - val_pi_loss: 0.8899 - val_v_loss: 0.2561\n",
      "Epoch 40/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.7277 - pi_loss: 0.3827 - v_loss: 0.3450 - val_loss: 1.1220 - val_pi_loss: 0.7958 - val_v_loss: 0.3262\n",
      "Epoch 41/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.7273 - pi_loss: 0.3855 - v_loss: 0.3418 - val_loss: 1.0883 - val_pi_loss: 0.7583 - val_v_loss: 0.3300\n",
      "Epoch 42/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.7344 - pi_loss: 0.3947 - v_loss: 0.3396 - val_loss: 1.0669 - val_pi_loss: 0.7328 - val_v_loss: 0.3342\n",
      "Epoch 43/100\n",
      "720/720 [==============================] - 6s 9ms/step - loss: 0.7133 - pi_loss: 0.3839 - v_loss: 0.3294 - val_loss: 1.0612 - val_pi_loss: 0.7227 - val_v_loss: 0.3385\n",
      "Epoch 44/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.7003 - pi_loss: 0.3740 - v_loss: 0.3263 - val_loss: 1.0377 - val_pi_loss: 0.6964 - val_v_loss: 0.3413\n",
      "Epoch 45/100\n",
      "720/720 [==============================] - 8s 11ms/step - loss: 0.7102 - pi_loss: 0.3859 - v_loss: 0.3243 - val_loss: 1.0438 - val_pi_loss: 0.7035 - val_v_loss: 0.3403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "720/720 [==============================] - 8s 11ms/step - loss: 0.6903 - pi_loss: 0.3676 - v_loss: 0.3227 - val_loss: 0.9505 - val_pi_loss: 0.6111 - val_v_loss: 0.3395\n",
      "Epoch 47/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.7109 - pi_loss: 0.3835 - v_loss: 0.3275 - val_loss: 0.9797 - val_pi_loss: 0.6422 - val_v_loss: 0.3375\n",
      "Epoch 48/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6858 - pi_loss: 0.3611 - v_loss: 0.3247 - val_loss: 0.9404 - val_pi_loss: 0.6022 - val_v_loss: 0.3382\n",
      "Epoch 49/100\n",
      "720/720 [==============================] - 7s 10ms/step - loss: 0.6773 - pi_loss: 0.3575 - v_loss: 0.3198 - val_loss: 0.8985 - val_pi_loss: 0.5598 - val_v_loss: 0.3387\n",
      "Epoch 50/100\n",
      "720/720 [==============================] - 7s 9ms/step - loss: 0.6848 - pi_loss: 0.3601 - v_loss: 0.3247 - val_loss: 0.9030 - val_pi_loss: 0.5630 - val_v_loss: 0.3399\n",
      "Epoch 51/100\n",
      "720/720 [==============================] - 7s 10ms/step - loss: 0.6827 - pi_loss: 0.3581 - v_loss: 0.3246 - val_loss: 0.9247 - val_pi_loss: 0.5883 - val_v_loss: 0.3364\n",
      "Epoch 52/100\n",
      "720/720 [==============================] - 10s 14ms/step - loss: 0.6784 - pi_loss: 0.3581 - v_loss: 0.3203 - val_loss: 0.8988 - val_pi_loss: 0.5593 - val_v_loss: 0.3394\n",
      "Epoch 53/100\n",
      "720/720 [==============================] - 8s 12ms/step - loss: 0.6679 - pi_loss: 0.3477 - v_loss: 0.3202 - val_loss: 0.8784 - val_pi_loss: 0.5433 - val_v_loss: 0.3351\n",
      "Epoch 54/100\n",
      "720/720 [==============================] - 7s 10ms/step - loss: 0.6641 - pi_loss: 0.3433 - v_loss: 0.3208 - val_loss: 0.8856 - val_pi_loss: 0.5478 - val_v_loss: 0.3379\n",
      "Epoch 55/100\n",
      "720/720 [==============================] - 5s 8ms/step - loss: 0.6615 - pi_loss: 0.3362 - v_loss: 0.3253 - val_loss: 0.8521 - val_pi_loss: 0.5180 - val_v_loss: 0.3341\n",
      "Epoch 56/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6613 - pi_loss: 0.3419 - v_loss: 0.3195 - val_loss: 0.8696 - val_pi_loss: 0.5317 - val_v_loss: 0.3379\n",
      "Epoch 57/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6552 - pi_loss: 0.3390 - v_loss: 0.3162 - val_loss: 0.8496 - val_pi_loss: 0.5165 - val_v_loss: 0.3331\n",
      "Epoch 58/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6528 - pi_loss: 0.3355 - v_loss: 0.3173 - val_loss: 0.8547 - val_pi_loss: 0.5202 - val_v_loss: 0.3345\n",
      "Epoch 59/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.6600 - pi_loss: 0.3417 - v_loss: 0.3183 - val_loss: 0.8545 - val_pi_loss: 0.5199 - val_v_loss: 0.3346\n",
      "Epoch 60/100\n",
      "720/720 [==============================] - 6s 9ms/step - loss: 0.6463 - pi_loss: 0.3320 - v_loss: 0.3142 - val_loss: 0.8295 - val_pi_loss: 0.4966 - val_v_loss: 0.3329\n",
      "Epoch 61/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.6415 - pi_loss: 0.3263 - v_loss: 0.3152 - val_loss: 0.8221 - val_pi_loss: 0.4893 - val_v_loss: 0.3328\n",
      "Epoch 62/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.6394 - pi_loss: 0.3276 - v_loss: 0.3118 - val_loss: 0.8262 - val_pi_loss: 0.4937 - val_v_loss: 0.3325\n",
      "Epoch 63/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.6532 - pi_loss: 0.3405 - v_loss: 0.3127 - val_loss: 0.8348 - val_pi_loss: 0.5014 - val_v_loss: 0.3334\n",
      "Epoch 64/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.6516 - pi_loss: 0.3362 - v_loss: 0.3154 - val_loss: 0.8120 - val_pi_loss: 0.4787 - val_v_loss: 0.3333\n",
      "Epoch 65/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6577 - pi_loss: 0.3407 - v_loss: 0.3170 - val_loss: 0.7957 - val_pi_loss: 0.4612 - val_v_loss: 0.3345\n",
      "Epoch 66/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6505 - pi_loss: 0.3334 - v_loss: 0.3171 - val_loss: 0.8225 - val_pi_loss: 0.4892 - val_v_loss: 0.3332\n",
      "Epoch 67/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.6439 - pi_loss: 0.3310 - v_loss: 0.3128 - val_loss: 0.8114 - val_pi_loss: 0.4716 - val_v_loss: 0.3398\n",
      "Epoch 68/100\n",
      "720/720 [==============================] - 6s 9ms/step - loss: 0.6354 - pi_loss: 0.3227 - v_loss: 0.3127 - val_loss: 0.7993 - val_pi_loss: 0.4637 - val_v_loss: 0.3356\n",
      "Epoch 69/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.6423 - pi_loss: 0.3330 - v_loss: 0.3093 - val_loss: 0.8082 - val_pi_loss: 0.4830 - val_v_loss: 0.3251\n",
      "Epoch 70/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.6323 - pi_loss: 0.3367 - v_loss: 0.2956 - val_loss: 0.7898 - val_pi_loss: 0.4984 - val_v_loss: 0.2913\n",
      "Epoch 71/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.6248 - pi_loss: 0.3340 - v_loss: 0.2908 - val_loss: 0.7453 - val_pi_loss: 0.4698 - val_v_loss: 0.2754\n",
      "Epoch 72/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.6038 - pi_loss: 0.3236 - v_loss: 0.2802 - val_loss: 0.7601 - val_pi_loss: 0.4840 - val_v_loss: 0.2761\n",
      "Epoch 73/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.6195 - pi_loss: 0.3395 - v_loss: 0.2799 - val_loss: 0.7801 - val_pi_loss: 0.4942 - val_v_loss: 0.2858\n",
      "Epoch 74/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.6053 - pi_loss: 0.3413 - v_loss: 0.2640 - val_loss: 0.7883 - val_pi_loss: 0.4936 - val_v_loss: 0.2947\n",
      "Epoch 75/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5595 - pi_loss: 0.3325 - v_loss: 0.2270 - val_loss: 0.7555 - val_pi_loss: 0.4930 - val_v_loss: 0.2625\n",
      "Epoch 76/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5502 - pi_loss: 0.3230 - v_loss: 0.2273 - val_loss: 0.7597 - val_pi_loss: 0.4843 - val_v_loss: 0.2754\n",
      "Epoch 77/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5511 - pi_loss: 0.3246 - v_loss: 0.2266 - val_loss: 0.7092 - val_pi_loss: 0.4643 - val_v_loss: 0.2449\n",
      "Epoch 78/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5507 - pi_loss: 0.3331 - v_loss: 0.2176 - val_loss: 0.7159 - val_pi_loss: 0.4669 - val_v_loss: 0.2490\n",
      "Epoch 79/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5478 - pi_loss: 0.3354 - v_loss: 0.2124 - val_loss: 0.7169 - val_pi_loss: 0.4738 - val_v_loss: 0.2430\n",
      "Epoch 80/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5359 - pi_loss: 0.3269 - v_loss: 0.2090 - val_loss: 0.7159 - val_pi_loss: 0.4764 - val_v_loss: 0.2395\n",
      "Epoch 81/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5584 - pi_loss: 0.3393 - v_loss: 0.2191 - val_loss: 0.7380 - val_pi_loss: 0.5047 - val_v_loss: 0.2333\n",
      "Epoch 82/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5502 - pi_loss: 0.3335 - v_loss: 0.2167 - val_loss: 0.7295 - val_pi_loss: 0.4826 - val_v_loss: 0.2469\n",
      "Epoch 83/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5462 - pi_loss: 0.3305 - v_loss: 0.2156 - val_loss: 0.7068 - val_pi_loss: 0.4686 - val_v_loss: 0.2382\n",
      "Epoch 84/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5391 - pi_loss: 0.3273 - v_loss: 0.2117 - val_loss: 0.7120 - val_pi_loss: 0.4674 - val_v_loss: 0.2446\n",
      "Epoch 85/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5575 - pi_loss: 0.3434 - v_loss: 0.2141 - val_loss: 0.7360 - val_pi_loss: 0.4912 - val_v_loss: 0.2447\n",
      "Epoch 86/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5392 - pi_loss: 0.3308 - v_loss: 0.2083 - val_loss: 0.6718 - val_pi_loss: 0.4584 - val_v_loss: 0.2134\n",
      "Epoch 87/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5233 - pi_loss: 0.3267 - v_loss: 0.1966 - val_loss: 0.7076 - val_pi_loss: 0.4669 - val_v_loss: 0.2407\n",
      "Epoch 88/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5370 - pi_loss: 0.3240 - v_loss: 0.2130 - val_loss: 0.7439 - val_pi_loss: 0.4904 - val_v_loss: 0.2535\n",
      "Epoch 89/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5269 - pi_loss: 0.3226 - v_loss: 0.2043 - val_loss: 0.7398 - val_pi_loss: 0.5085 - val_v_loss: 0.2313\n",
      "Epoch 90/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5072 - pi_loss: 0.3205 - v_loss: 0.1867 - val_loss: 0.7043 - val_pi_loss: 0.4821 - val_v_loss: 0.2222\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5102 - pi_loss: 0.3187 - v_loss: 0.1915 - val_loss: 0.7042 - val_pi_loss: 0.4735 - val_v_loss: 0.2307\n",
      "Epoch 92/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5004 - pi_loss: 0.3173 - v_loss: 0.1831 - val_loss: 0.6889 - val_pi_loss: 0.4806 - val_v_loss: 0.2083\n",
      "Epoch 93/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5247 - pi_loss: 0.3312 - v_loss: 0.1936 - val_loss: 0.7305 - val_pi_loss: 0.4802 - val_v_loss: 0.2503\n",
      "Epoch 94/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5282 - pi_loss: 0.3283 - v_loss: 0.1999 - val_loss: 0.7479 - val_pi_loss: 0.4846 - val_v_loss: 0.2633\n",
      "Epoch 95/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5239 - pi_loss: 0.3272 - v_loss: 0.1967 - val_loss: 0.7361 - val_pi_loss: 0.4815 - val_v_loss: 0.2547\n",
      "Epoch 96/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.5045 - pi_loss: 0.3198 - v_loss: 0.1847 - val_loss: 0.7892 - val_pi_loss: 0.5150 - val_v_loss: 0.2743\n",
      "Epoch 97/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.4930 - pi_loss: 0.3236 - v_loss: 0.1693 - val_loss: 0.7556 - val_pi_loss: 0.4834 - val_v_loss: 0.2722\n",
      "Epoch 98/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.5054 - pi_loss: 0.3369 - v_loss: 0.1685 - val_loss: 0.7146 - val_pi_loss: 0.4949 - val_v_loss: 0.2197\n",
      "Epoch 99/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.4674 - pi_loss: 0.3174 - v_loss: 0.1500 - val_loss: 0.7085 - val_pi_loss: 0.4901 - val_v_loss: 0.2184\n",
      "Epoch 100/100\n",
      "720/720 [==============================] - 7s 10ms/step - loss: 0.4866 - pi_loss: 0.3233 - v_loss: 0.1633 - val_loss: 0.6916 - val_pi_loss: 0.4836 - val_v_loss: 0.2081\n",
      "Assessing new...\n",
      "\n",
      "################################### - RESULTS - ###################################\n",
      "Tie    25\n",
      "New     4\n",
      "Old     1\n",
      "dtype: int64\n",
      "NEW GENERATION SUPERIOR\n",
      "Updating model.\n",
      "###################################################################################\n",
      "\n",
      "Saved model to disk\n",
      "Assessing old...\n",
      "Dataset Size 900\n",
      "Training...\n",
      "Train on 720 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "720/720 [==============================] - 7s 9ms/step - loss: 0.4804 - pi_loss: 0.3285 - v_loss: 0.1519 - val_loss: 0.4281 - val_pi_loss: 0.3076 - val_v_loss: 0.1205\n",
      "Epoch 2/100\n",
      "720/720 [==============================] - 6s 8ms/step - loss: 0.4404 - pi_loss: 0.3067 - v_loss: 0.1338 - val_loss: 0.4300 - val_pi_loss: 0.3171 - val_v_loss: 0.1128\n",
      "Epoch 3/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.4498 - pi_loss: 0.2930 - v_loss: 0.1568 - val_loss: 0.4373 - val_pi_loss: 0.3281 - val_v_loss: 0.1091\n",
      "Epoch 4/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.4182 - pi_loss: 0.2862 - v_loss: 0.1319 - val_loss: 0.4485 - val_pi_loss: 0.2991 - val_v_loss: 0.1495\n",
      "Epoch 5/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3649 - pi_loss: 0.2475 - v_loss: 0.1175 - val_loss: 0.3952 - val_pi_loss: 0.2898 - val_v_loss: 0.1055\n",
      "Epoch 6/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3775 - pi_loss: 0.2495 - v_loss: 0.1280 - val_loss: 0.4224 - val_pi_loss: 0.3167 - val_v_loss: 0.1056\n",
      "Epoch 7/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3863 - pi_loss: 0.2615 - v_loss: 0.1248 - val_loss: 0.4277 - val_pi_loss: 0.3083 - val_v_loss: 0.1194\n",
      "Epoch 8/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3992 - pi_loss: 0.2734 - v_loss: 0.1258 - val_loss: 0.4053 - val_pi_loss: 0.3094 - val_v_loss: 0.0959\n",
      "Epoch 9/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3928 - pi_loss: 0.2748 - v_loss: 0.1181 - val_loss: 0.4956 - val_pi_loss: 0.3874 - val_v_loss: 0.1081\n",
      "Epoch 10/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.3727 - pi_loss: 0.2608 - v_loss: 0.1118 - val_loss: 0.4228 - val_pi_loss: 0.3294 - val_v_loss: 0.0934\n",
      "Epoch 11/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.3611 - pi_loss: 0.2455 - v_loss: 0.1156 - val_loss: 0.4113 - val_pi_loss: 0.3213 - val_v_loss: 0.0901\n",
      "Epoch 12/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3801 - pi_loss: 0.2586 - v_loss: 0.1215 - val_loss: 0.6799 - val_pi_loss: 0.3310 - val_v_loss: 0.3489\n",
      "Epoch 13/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3989 - pi_loss: 0.2405 - v_loss: 0.1584 - val_loss: 0.4962 - val_pi_loss: 0.3309 - val_v_loss: 0.1653\n",
      "Epoch 14/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3749 - pi_loss: 0.2347 - v_loss: 0.1402 - val_loss: 0.6056 - val_pi_loss: 0.3402 - val_v_loss: 0.2654\n",
      "Epoch 15/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.3690 - pi_loss: 0.2356 - v_loss: 0.1334 - val_loss: 0.4447 - val_pi_loss: 0.3076 - val_v_loss: 0.1371\n",
      "Epoch 16/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3593 - pi_loss: 0.2340 - v_loss: 0.1253 - val_loss: 0.4284 - val_pi_loss: 0.3046 - val_v_loss: 0.1238\n",
      "Epoch 17/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3546 - pi_loss: 0.2325 - v_loss: 0.1221 - val_loss: 0.4185 - val_pi_loss: 0.2993 - val_v_loss: 0.1193\n",
      "Epoch 18/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3386 - pi_loss: 0.2195 - v_loss: 0.1191 - val_loss: 0.3900 - val_pi_loss: 0.2836 - val_v_loss: 0.1064\n",
      "Epoch 19/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3353 - pi_loss: 0.2194 - v_loss: 0.1159 - val_loss: 0.3938 - val_pi_loss: 0.2922 - val_v_loss: 0.1016\n",
      "Epoch 20/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3408 - pi_loss: 0.2206 - v_loss: 0.1202 - val_loss: 0.4356 - val_pi_loss: 0.3114 - val_v_loss: 0.1243\n",
      "Epoch 21/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3645 - pi_loss: 0.2285 - v_loss: 0.1360 - val_loss: 0.3989 - val_pi_loss: 0.3004 - val_v_loss: 0.0985\n",
      "Epoch 22/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3475 - pi_loss: 0.2284 - v_loss: 0.1192 - val_loss: 0.4158 - val_pi_loss: 0.3159 - val_v_loss: 0.0999\n",
      "Epoch 23/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3349 - pi_loss: 0.2185 - v_loss: 0.1164 - val_loss: 0.4194 - val_pi_loss: 0.3146 - val_v_loss: 0.1048\n",
      "Epoch 24/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3442 - pi_loss: 0.2310 - v_loss: 0.1132 - val_loss: 0.4040 - val_pi_loss: 0.3032 - val_v_loss: 0.1008\n",
      "Epoch 25/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3401 - pi_loss: 0.2285 - v_loss: 0.1116 - val_loss: 0.4001 - val_pi_loss: 0.3067 - val_v_loss: 0.0934\n",
      "Epoch 26/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3279 - pi_loss: 0.2210 - v_loss: 0.1069 - val_loss: 0.3851 - val_pi_loss: 0.2948 - val_v_loss: 0.0903\n",
      "Epoch 27/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3555 - pi_loss: 0.2310 - v_loss: 0.1246 - val_loss: 0.4158 - val_pi_loss: 0.3130 - val_v_loss: 0.1029\n",
      "Epoch 28/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3218 - pi_loss: 0.2156 - v_loss: 0.1063 - val_loss: 0.3817 - val_pi_loss: 0.2946 - val_v_loss: 0.0871\n",
      "Epoch 29/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3165 - pi_loss: 0.2122 - v_loss: 0.1043 - val_loss: 0.3742 - val_pi_loss: 0.2855 - val_v_loss: 0.0887\n",
      "Epoch 30/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3060 - pi_loss: 0.2008 - v_loss: 0.1053 - val_loss: 0.3821 - val_pi_loss: 0.2848 - val_v_loss: 0.0973\n",
      "Epoch 31/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3161 - pi_loss: 0.2052 - v_loss: 0.1109 - val_loss: 0.3562 - val_pi_loss: 0.2694 - val_v_loss: 0.0867\n",
      "Epoch 32/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3015 - pi_loss: 0.2059 - v_loss: 0.0957 - val_loss: 0.3650 - val_pi_loss: 0.2798 - val_v_loss: 0.0852\n",
      "Epoch 33/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3289 - pi_loss: 0.2161 - v_loss: 0.1128 - val_loss: 0.4119 - val_pi_loss: 0.3233 - val_v_loss: 0.0886\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3052 - pi_loss: 0.2024 - v_loss: 0.1028 - val_loss: 0.3702 - val_pi_loss: 0.2788 - val_v_loss: 0.0914\n",
      "Epoch 35/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3317 - pi_loss: 0.2158 - v_loss: 0.1159 - val_loss: 0.3698 - val_pi_loss: 0.2800 - val_v_loss: 0.0899\n",
      "Epoch 36/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3076 - pi_loss: 0.2151 - v_loss: 0.0925 - val_loss: 0.4096 - val_pi_loss: 0.3176 - val_v_loss: 0.0920\n",
      "Epoch 37/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3002 - pi_loss: 0.2076 - v_loss: 0.0926 - val_loss: 0.4024 - val_pi_loss: 0.3165 - val_v_loss: 0.0860\n",
      "Epoch 38/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2736 - pi_loss: 0.1996 - v_loss: 0.0740 - val_loss: 0.4850 - val_pi_loss: 0.3066 - val_v_loss: 0.1784\n",
      "Epoch 39/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3113 - pi_loss: 0.2080 - v_loss: 0.1033 - val_loss: 0.3773 - val_pi_loss: 0.3011 - val_v_loss: 0.0762\n",
      "Epoch 40/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3138 - pi_loss: 0.2166 - v_loss: 0.0972 - val_loss: 0.4156 - val_pi_loss: 0.3362 - val_v_loss: 0.0794\n",
      "Epoch 41/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.3314 - pi_loss: 0.2193 - v_loss: 0.1121 - val_loss: 0.4199 - val_pi_loss: 0.3182 - val_v_loss: 0.1017\n",
      "Epoch 42/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3312 - pi_loss: 0.2132 - v_loss: 0.1179 - val_loss: 0.4203 - val_pi_loss: 0.3238 - val_v_loss: 0.0966\n",
      "Epoch 43/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3490 - pi_loss: 0.2272 - v_loss: 0.1217 - val_loss: 0.4258 - val_pi_loss: 0.3279 - val_v_loss: 0.0979\n",
      "Epoch 44/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3361 - pi_loss: 0.2344 - v_loss: 0.1017 - val_loss: 0.3857 - val_pi_loss: 0.3217 - val_v_loss: 0.0640\n",
      "Epoch 45/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3237 - pi_loss: 0.2189 - v_loss: 0.1049 - val_loss: 0.3759 - val_pi_loss: 0.2984 - val_v_loss: 0.0775\n",
      "Epoch 46/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3184 - pi_loss: 0.2160 - v_loss: 0.1023 - val_loss: 0.3910 - val_pi_loss: 0.2993 - val_v_loss: 0.0917\n",
      "Epoch 47/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.2943 - pi_loss: 0.2015 - v_loss: 0.0927 - val_loss: 0.3334 - val_pi_loss: 0.2752 - val_v_loss: 0.0583\n",
      "Epoch 48/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2957 - pi_loss: 0.2102 - v_loss: 0.0855 - val_loss: 0.3526 - val_pi_loss: 0.3005 - val_v_loss: 0.0521\n",
      "Epoch 49/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.3028 - pi_loss: 0.2073 - v_loss: 0.0955 - val_loss: 0.3780 - val_pi_loss: 0.2799 - val_v_loss: 0.0980\n",
      "Epoch 50/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3216 - pi_loss: 0.2129 - v_loss: 0.1087 - val_loss: 0.3884 - val_pi_loss: 0.2871 - val_v_loss: 0.1013\n",
      "Epoch 51/100\n",
      "720/720 [==============================] - 5s 6ms/step - loss: 0.2845 - pi_loss: 0.1978 - v_loss: 0.0867 - val_loss: 0.4857 - val_pi_loss: 0.3231 - val_v_loss: 0.1626\n",
      "Epoch 52/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3037 - pi_loss: 0.2065 - v_loss: 0.0972 - val_loss: 0.3913 - val_pi_loss: 0.2932 - val_v_loss: 0.0981\n",
      "Epoch 53/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3086 - pi_loss: 0.1979 - v_loss: 0.1107 - val_loss: 0.4101 - val_pi_loss: 0.3058 - val_v_loss: 0.1043\n",
      "Epoch 54/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3179 - pi_loss: 0.2021 - v_loss: 0.1158 - val_loss: 0.3934 - val_pi_loss: 0.2922 - val_v_loss: 0.1013\n",
      "Epoch 55/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2912 - pi_loss: 0.2017 - v_loss: 0.0895 - val_loss: 0.3439 - val_pi_loss: 0.2832 - val_v_loss: 0.0607\n",
      "Epoch 56/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3116 - pi_loss: 0.2007 - v_loss: 0.1109 - val_loss: 0.3870 - val_pi_loss: 0.2899 - val_v_loss: 0.0971\n",
      "Epoch 57/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2748 - pi_loss: 0.2009 - v_loss: 0.0739 - val_loss: 0.3906 - val_pi_loss: 0.3020 - val_v_loss: 0.0886\n",
      "Epoch 58/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3047 - pi_loss: 0.2033 - v_loss: 0.1014 - val_loss: 0.3596 - val_pi_loss: 0.2750 - val_v_loss: 0.0846\n",
      "Epoch 59/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2971 - pi_loss: 0.2103 - v_loss: 0.0868 - val_loss: 0.3740 - val_pi_loss: 0.2953 - val_v_loss: 0.0787\n",
      "Epoch 60/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2981 - pi_loss: 0.2091 - v_loss: 0.0890 - val_loss: 0.3892 - val_pi_loss: 0.2999 - val_v_loss: 0.0893\n",
      "Epoch 61/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2881 - pi_loss: 0.1928 - v_loss: 0.0952 - val_loss: 0.3792 - val_pi_loss: 0.2861 - val_v_loss: 0.0932\n",
      "Epoch 62/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2942 - pi_loss: 0.2007 - v_loss: 0.0934 - val_loss: 0.3806 - val_pi_loss: 0.2947 - val_v_loss: 0.0859\n",
      "Epoch 63/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2747 - pi_loss: 0.1986 - v_loss: 0.0761 - val_loss: 0.3765 - val_pi_loss: 0.3161 - val_v_loss: 0.0604\n",
      "Epoch 64/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3013 - pi_loss: 0.2209 - v_loss: 0.0804 - val_loss: 0.3693 - val_pi_loss: 0.2965 - val_v_loss: 0.0728\n",
      "Epoch 65/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3013 - pi_loss: 0.2081 - v_loss: 0.0932 - val_loss: 0.3990 - val_pi_loss: 0.3133 - val_v_loss: 0.0857\n",
      "Epoch 66/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2908 - pi_loss: 0.2021 - v_loss: 0.0887 - val_loss: 0.3837 - val_pi_loss: 0.2962 - val_v_loss: 0.0875\n",
      "Epoch 67/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2976 - pi_loss: 0.1991 - v_loss: 0.0985 - val_loss: 0.3785 - val_pi_loss: 0.2844 - val_v_loss: 0.0941\n",
      "Epoch 68/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2932 - pi_loss: 0.2029 - v_loss: 0.0903 - val_loss: 0.3762 - val_pi_loss: 0.2902 - val_v_loss: 0.0860\n",
      "Epoch 69/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2663 - pi_loss: 0.1903 - v_loss: 0.0760 - val_loss: 0.3269 - val_pi_loss: 0.2804 - val_v_loss: 0.0465\n",
      "Epoch 70/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2704 - pi_loss: 0.1881 - v_loss: 0.0823 - val_loss: 0.3570 - val_pi_loss: 0.2707 - val_v_loss: 0.0863\n",
      "Epoch 71/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2897 - pi_loss: 0.1900 - v_loss: 0.0997 - val_loss: 0.3570 - val_pi_loss: 0.2764 - val_v_loss: 0.0806\n",
      "Epoch 72/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2492 - pi_loss: 0.1850 - v_loss: 0.0642 - val_loss: 0.3733 - val_pi_loss: 0.2942 - val_v_loss: 0.0790\n",
      "Epoch 73/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2598 - pi_loss: 0.1859 - v_loss: 0.0739 - val_loss: 0.3872 - val_pi_loss: 0.3016 - val_v_loss: 0.0856\n",
      "Epoch 74/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2432 - pi_loss: 0.1880 - v_loss: 0.0552 - val_loss: 0.3336 - val_pi_loss: 0.2851 - val_v_loss: 0.0485\n",
      "Epoch 75/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2510 - pi_loss: 0.1939 - v_loss: 0.0571 - val_loss: 0.3530 - val_pi_loss: 0.3069 - val_v_loss: 0.0461\n",
      "Epoch 76/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3072 - pi_loss: 0.2447 - v_loss: 0.0625 - val_loss: 0.3805 - val_pi_loss: 0.3287 - val_v_loss: 0.0518\n",
      "Epoch 77/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2636 - pi_loss: 0.2080 - v_loss: 0.0555 - val_loss: 0.3885 - val_pi_loss: 0.3323 - val_v_loss: 0.0561\n",
      "Epoch 78/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2692 - pi_loss: 0.1982 - v_loss: 0.0710 - val_loss: 0.4097 - val_pi_loss: 0.3298 - val_v_loss: 0.0799\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2466 - pi_loss: 0.1901 - v_loss: 0.0565 - val_loss: 0.3857 - val_pi_loss: 0.3056 - val_v_loss: 0.0801\n",
      "Epoch 80/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2638 - pi_loss: 0.1948 - v_loss: 0.0690 - val_loss: 0.3918 - val_pi_loss: 0.2931 - val_v_loss: 0.0987\n",
      "Epoch 81/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2891 - pi_loss: 0.1991 - v_loss: 0.0900 - val_loss: 0.4046 - val_pi_loss: 0.3164 - val_v_loss: 0.0882\n",
      "Epoch 82/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2899 - pi_loss: 0.1969 - v_loss: 0.0930 - val_loss: 0.3951 - val_pi_loss: 0.3105 - val_v_loss: 0.0847\n",
      "Epoch 83/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2802 - pi_loss: 0.1901 - v_loss: 0.0901 - val_loss: 0.4003 - val_pi_loss: 0.3110 - val_v_loss: 0.0892\n",
      "Epoch 84/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2716 - pi_loss: 0.1857 - v_loss: 0.0859 - val_loss: 0.3899 - val_pi_loss: 0.3077 - val_v_loss: 0.0822\n",
      "Epoch 85/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2783 - pi_loss: 0.1901 - v_loss: 0.0882 - val_loss: 0.3874 - val_pi_loss: 0.2996 - val_v_loss: 0.0878\n",
      "Epoch 86/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2564 - pi_loss: 0.1844 - v_loss: 0.0720 - val_loss: 0.3589 - val_pi_loss: 0.2910 - val_v_loss: 0.0680\n",
      "Epoch 87/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2653 - pi_loss: 0.1926 - v_loss: 0.0726 - val_loss: 0.3559 - val_pi_loss: 0.3091 - val_v_loss: 0.0468\n",
      "Epoch 88/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2397 - pi_loss: 0.1893 - v_loss: 0.0504 - val_loss: 0.3520 - val_pi_loss: 0.3110 - val_v_loss: 0.0410\n",
      "Epoch 89/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2328 - pi_loss: 0.1913 - v_loss: 0.0415 - val_loss: 0.3471 - val_pi_loss: 0.2952 - val_v_loss: 0.0519\n",
      "Epoch 90/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2604 - pi_loss: 0.2036 - v_loss: 0.0568 - val_loss: 0.3214 - val_pi_loss: 0.2794 - val_v_loss: 0.0420\n",
      "Epoch 91/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2635 - pi_loss: 0.2104 - v_loss: 0.0530 - val_loss: 0.3994 - val_pi_loss: 0.3060 - val_v_loss: 0.0934\n",
      "Epoch 92/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2842 - pi_loss: 0.2004 - v_loss: 0.0838 - val_loss: 0.3546 - val_pi_loss: 0.2958 - val_v_loss: 0.0588\n",
      "Epoch 93/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2499 - pi_loss: 0.1937 - v_loss: 0.0562 - val_loss: 0.3914 - val_pi_loss: 0.2963 - val_v_loss: 0.0951\n",
      "Epoch 94/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2483 - pi_loss: 0.1873 - v_loss: 0.0610 - val_loss: 0.3573 - val_pi_loss: 0.3137 - val_v_loss: 0.0437\n",
      "Epoch 95/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2402 - pi_loss: 0.1852 - v_loss: 0.0549 - val_loss: 0.3726 - val_pi_loss: 0.3060 - val_v_loss: 0.0665\n",
      "Epoch 96/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2515 - pi_loss: 0.1867 - v_loss: 0.0649 - val_loss: 0.3406 - val_pi_loss: 0.2957 - val_v_loss: 0.0448\n",
      "Epoch 97/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2321 - pi_loss: 0.1869 - v_loss: 0.0452 - val_loss: 0.3970 - val_pi_loss: 0.3224 - val_v_loss: 0.0746\n",
      "Epoch 98/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.2438 - pi_loss: 0.1853 - v_loss: 0.0585 - val_loss: 0.3512 - val_pi_loss: 0.2975 - val_v_loss: 0.0538\n",
      "Epoch 99/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2491 - pi_loss: 0.1866 - v_loss: 0.0625 - val_loss: 0.3731 - val_pi_loss: 0.3016 - val_v_loss: 0.0715\n",
      "Epoch 100/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.2338 - pi_loss: 0.1816 - v_loss: 0.0522 - val_loss: 0.3749 - val_pi_loss: 0.3148 - val_v_loss: 0.0601\n",
      "Assessing new...\n",
      "\n",
      "################################### - RESULTS - ###################################\n",
      "Tie    24\n",
      "Old     6\n",
      "dtype: int64\n",
      "New Generation INFERIOR\n",
      "Rejecting model.\n",
      "Loaded model from disk\n",
      "###################################################################################\n",
      "\n",
      "Saved model to disk\n",
      "Assessing old...\n",
      "Dataset Size 900\n",
      "Training...\n",
      "Train on 720 samples, validate on 180 samples\n",
      "Epoch 1/100\n",
      "720/720 [==============================] - 6s 9ms/step - loss: 0.5820 - pi_loss: 0.3579 - v_loss: 0.2242 - val_loss: 0.6463 - val_pi_loss: 0.3318 - val_v_loss: 0.3145\n",
      "Epoch 2/100\n",
      "720/720 [==============================] - 3s 5ms/step - loss: 0.5215 - pi_loss: 0.3434 - v_loss: 0.1780 - val_loss: 0.6998 - val_pi_loss: 0.3784 - val_v_loss: 0.3214\n",
      "Epoch 3/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.4743 - pi_loss: 0.3240 - v_loss: 0.1503 - val_loss: 0.5715 - val_pi_loss: 0.3553 - val_v_loss: 0.2162\n",
      "Epoch 4/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.4417 - pi_loss: 0.3058 - v_loss: 0.1358 - val_loss: 0.5551 - val_pi_loss: 0.3814 - val_v_loss: 0.1737\n",
      "Epoch 5/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.4121 - pi_loss: 0.2842 - v_loss: 0.1279 - val_loss: 0.4736 - val_pi_loss: 0.3329 - val_v_loss: 0.1407\n",
      "Epoch 6/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.4177 - pi_loss: 0.2802 - v_loss: 0.1375 - val_loss: 0.4918 - val_pi_loss: 0.3108 - val_v_loss: 0.1811\n",
      "Epoch 7/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3817 - pi_loss: 0.2530 - v_loss: 0.1288 - val_loss: 0.5987 - val_pi_loss: 0.3293 - val_v_loss: 0.2694\n",
      "Epoch 8/100\n",
      "720/720 [==============================] - 4s 5ms/step - loss: 0.3710 - pi_loss: 0.2463 - v_loss: 0.1247 - val_loss: 0.4795 - val_pi_loss: 0.2765 - val_v_loss: 0.2030\n",
      "Epoch 9/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3582 - pi_loss: 0.2301 - v_loss: 0.1281 - val_loss: 0.5345 - val_pi_loss: 0.3532 - val_v_loss: 0.1814\n",
      "Epoch 10/100\n",
      "720/720 [==============================] - 4s 6ms/step - loss: 0.3429 - pi_loss: 0.2271 - v_loss: 0.1158 - val_loss: 0.4786 - val_pi_loss: 0.2948 - val_v_loss: 0.1837\n",
      "Epoch 11/100\n",
      "720/720 [==============================] - 5s 7ms/step - loss: 0.3606 - pi_loss: 0.2271 - v_loss: 0.1335 - val_loss: 0.4962 - val_pi_loss: 0.2749 - val_v_loss: 0.2213\n",
      "Epoch 12/100\n",
      "256/720 [=========>....................] - ETA: 3s - loss: 0.3135 - pi_loss: 0.1921 - v_loss: 0.1214"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ae7f6938d669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marena\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Columbia/Semester 9/Deep Learning/Project/AlphaNP/SELFPLAY.py\u001b[0m in \u001b[0;36marena\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'numGens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mcounts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Columbia/Semester 9/Deep Learning/Project/AlphaNP/SELFPLAY.py\u001b[0m in \u001b[0;36mnext_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset Size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Assessing new...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Columbia/Semester 9/Deep Learning/Project/AlphaNP/NETS.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         hist = self.model.fit(x = [input], y = [target_pis, target_vs], validation_split=self.args['validation_split'],\n\u001b[0;32m---> 45\u001b[0;31m                        batch_size = self.args.batch_size, epochs = self.args.epochs, verbose=1, callbacks=[es])\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c.arena()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "rnn.load_model('Generation_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/20\n",
      "240/240 [==============================] - 4s 17ms/step - loss: 1.9883 - pi_loss: 1.4030 - v_loss: 0.5853 - val_loss: 1.8350 - val_pi_loss: 1.2948 - val_v_loss: 0.5401\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.8370 - pi_loss: 1.2518 - v_loss: 0.5853 - val_loss: 1.7067 - val_pi_loss: 1.1666 - val_v_loss: 0.5401\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.7253 - pi_loss: 1.1400 - v_loss: 0.5853 - val_loss: 1.6573 - val_pi_loss: 1.1172 - val_v_loss: 0.5401\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.7005 - pi_loss: 1.1153 - v_loss: 0.5853 - val_loss: 1.6542 - val_pi_loss: 1.1141 - val_v_loss: 0.5401\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.6850 - pi_loss: 1.0997 - v_loss: 0.5853 - val_loss: 1.6249 - val_pi_loss: 1.0847 - val_v_loss: 0.5401\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.6540 - pi_loss: 1.0687 - v_loss: 0.5853 - val_loss: 1.5933 - val_pi_loss: 1.0532 - val_v_loss: 0.5401\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.6312 - pi_loss: 1.0460 - v_loss: 0.5853 - val_loss: 1.5741 - val_pi_loss: 1.0340 - val_v_loss: 0.5401\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.6131 - pi_loss: 1.0278 - v_loss: 0.5853 - val_loss: 1.5553 - val_pi_loss: 1.0152 - val_v_loss: 0.5401\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.5845 - pi_loss: 0.9992 - v_loss: 0.5853 - val_loss: 1.5308 - val_pi_loss: 0.9907 - val_v_loss: 0.5401\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.5741 - pi_loss: 0.9889 - v_loss: 0.5853 - val_loss: 1.5057 - val_pi_loss: 0.9656 - val_v_loss: 0.5401\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.5385 - pi_loss: 0.9532 - v_loss: 0.5853 - val_loss: 1.4766 - val_pi_loss: 0.9365 - val_v_loss: 0.5401\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.5044 - pi_loss: 0.9191 - v_loss: 0.5853 - val_loss: 1.4446 - val_pi_loss: 0.9044 - val_v_loss: 0.5401\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.4867 - pi_loss: 0.9014 - v_loss: 0.5853 - val_loss: 1.4112 - val_pi_loss: 0.8711 - val_v_loss: 0.5401\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.4486 - pi_loss: 0.8634 - v_loss: 0.5853 - val_loss: 1.3772 - val_pi_loss: 0.8371 - val_v_loss: 0.5401\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.4106 - pi_loss: 0.8253 - v_loss: 0.5853 - val_loss: 1.3423 - val_pi_loss: 0.8022 - val_v_loss: 0.5401\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.3746 - pi_loss: 0.7893 - v_loss: 0.5853 - val_loss: 1.3088 - val_pi_loss: 0.7687 - val_v_loss: 0.5401\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.3539 - pi_loss: 0.7687 - v_loss: 0.5853 - val_loss: 1.2796 - val_pi_loss: 0.7395 - val_v_loss: 0.5401\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.3199 - pi_loss: 0.7346 - v_loss: 0.5853 - val_loss: 1.2391 - val_pi_loss: 0.6990 - val_v_loss: 0.5401\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2711 - pi_loss: 0.6858 - v_loss: 0.5853 - val_loss: 1.2119 - val_pi_loss: 0.6718 - val_v_loss: 0.5401\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2834 - pi_loss: 0.7085 - v_loss: 0.57 - 0s 652us/step - loss: 1.2703 - pi_loss: 0.6850 - v_loss: 0.5853 - val_loss: 1.1768 - val_pi_loss: 0.6367 - val_v_loss: 0.5401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17f8d6f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.create_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/20\n",
      "240/240 [==============================] - 4s 15ms/step - loss: 2.7657 - pi_loss: 2.1804 - v_loss: 0.5853 - val_loss: 2.5057 - val_pi_loss: 1.9656 - val_v_loss: 0.5401\n",
      "Epoch 2/20\n",
      "240/240 [==============================] - 0s 614us/step - loss: 2.4479 - pi_loss: 1.8626 - v_loss: 0.5853 - val_loss: 2.2166 - val_pi_loss: 1.6765 - val_v_loss: 0.5401\n",
      "Epoch 3/20\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.1761 - pi_loss: 1.5909 - v_loss: 0.5853 - val_loss: 1.9763 - val_pi_loss: 1.4362 - val_v_loss: 0.5401\n",
      "Epoch 4/20\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.9523 - pi_loss: 1.3670 - v_loss: 0.5853 - val_loss: 1.7794 - val_pi_loss: 1.2393 - val_v_loss: 0.5401\n",
      "Epoch 5/20\n",
      "240/240 [==============================] - 0s 610us/step - loss: 1.7840 - pi_loss: 1.1987 - v_loss: 0.5853 - val_loss: 1.6765 - val_pi_loss: 1.1364 - val_v_loss: 0.5401\n",
      "Epoch 6/20\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6973 - pi_loss: 1.1120 - v_loss: 0.5853 - val_loss: 1.6725 - val_pi_loss: 1.1324 - val_v_loss: 0.5401\n",
      "Epoch 7/20\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.7119 - pi_loss: 1.1266 - v_loss: 0.5853 - val_loss: 1.6295 - val_pi_loss: 1.0894 - val_v_loss: 0.5401\n",
      "Epoch 8/20\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.6593 - pi_loss: 1.0741 - v_loss: 0.5853 - val_loss: 1.6020 - val_pi_loss: 1.0619 - val_v_loss: 0.5401\n",
      "Epoch 9/20\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6420 - pi_loss: 1.0567 - v_loss: 0.5853 - val_loss: 1.5942 - val_pi_loss: 1.0541 - val_v_loss: 0.5401\n",
      "Epoch 10/20\n",
      "240/240 [==============================] - 0s 616us/step - loss: 1.6309 - pi_loss: 1.0456 - v_loss: 0.5853 - val_loss: 1.5771 - val_pi_loss: 1.0370 - val_v_loss: 0.5401\n",
      "Epoch 11/20\n",
      "240/240 [==============================] - 0s 606us/step - loss: 1.6140 - pi_loss: 1.0288 - v_loss: 0.5853 - val_loss: 1.5626 - val_pi_loss: 1.0225 - val_v_loss: 0.5401\n",
      "Epoch 12/20\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.5981 - pi_loss: 1.0128 - v_loss: 0.5853 - val_loss: 1.5498 - val_pi_loss: 1.0097 - val_v_loss: 0.5401\n",
      "Epoch 13/20\n",
      "240/240 [==============================] - 0s 604us/step - loss: 1.5840 - pi_loss: 0.9988 - v_loss: 0.5853 - val_loss: 1.5276 - val_pi_loss: 0.9875 - val_v_loss: 0.5401\n",
      "Epoch 14/20\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.5643 - pi_loss: 0.9790 - v_loss: 0.5853 - val_loss: 1.5040 - val_pi_loss: 0.9639 - val_v_loss: 0.5401\n",
      "Epoch 15/20\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.5364 - pi_loss: 0.9511 - v_loss: 0.5853 - val_loss: 1.4793 - val_pi_loss: 0.9392 - val_v_loss: 0.5401\n",
      "Epoch 16/20\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.5153 - pi_loss: 0.9301 - v_loss: 0.5853 - val_loss: 1.4513 - val_pi_loss: 0.9112 - val_v_loss: 0.5401\n",
      "Epoch 17/20\n",
      "240/240 [==============================] - 0s 591us/step - loss: 1.4918 - pi_loss: 0.9065 - v_loss: 0.5853 - val_loss: 1.4231 - val_pi_loss: 0.8830 - val_v_loss: 0.5401\n",
      "Epoch 18/20\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.4503 - pi_loss: 0.8651 - v_loss: 0.5853 - val_loss: 1.3911 - val_pi_loss: 0.8510 - val_v_loss: 0.5401\n",
      "Epoch 19/20\n",
      "240/240 [==============================] - 0s 789us/step - loss: 1.4332 - pi_loss: 0.8479 - v_loss: 0.5853 - val_loss: 1.3616 - val_pi_loss: 0.8215 - val_v_loss: 0.5401\n",
      "Epoch 20/20\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.4192 - pi_loss: 0.8340 - v_loss: 0.5853 - val_loss: 1.3288 - val_pi_loss: 0.7887 - val_v_loss: 0.5401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17dfb6cc0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
